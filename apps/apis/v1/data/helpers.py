from datetime import datetime
import csv
import anthropic

from fastapi import HTTPException, status

from apps.common.exception import DuplicateTimestampError
from apps.config.settings import API_KEY
from apps.apis.v1.data.prompt import GENERATION_PROMPT, TIMESERIES_PROMPT

API_ENDPOINT = "https://api.claude.ai/v3/generate"


def generate_synthetic_data(example_data, num_rows, data_type, batch_size):
    if data_type == "timeseries":
        prompt = TIMESERIES_PROMPT
    else:
        prompt = GENERATION_PROMPT

    try:
        client = anthropic.Client(api_key=API_KEY)
        structured_data = []
        fieldnames = example_data.strip().split("\n")[0].split(",")

        num_batches = (num_rows + batch_size - 1) // batch_size

        for i in range(num_batches):
            start_index = i * batch_size
            end_index = min((i + 1) * batch_size, num_rows)
            batch_num_rows = end_index - start_index

            batch_prompt = prompt.format(num_rows=batch_num_rows)

            try:
                response = client.messages.create(
                    model="claude-3-sonnet-20240229",
                    max_tokens=1024,
                    messages=[
                        {"role": "user", "content": f"{anthropic.HUMAN_PROMPT} Example data:\n{example_data}\n\n{batch_prompt} {anthropic.AI_PROMPT}"}
                    ]
                )
                generated_data = response.content[0].text
                if not generated_data:
                    raise ValueError("No data was generated by the API.")
            except Exception as e:
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail=f"An error occurred while calling the API: {e}"
                ) from e

            print(f"API response: {response}")  # Print the API response for debugging

            # Remove any conversational text before the CSV data
            csv_start_index = generated_data.find("\n")
            if csv_start_index != -1:
                generated_data = generated_data[csv_start_index + 1:]

            rows = generated_data.strip().split("\n")
            for row in rows:
                values = row.split(",")
                if len(values) == len(fieldnames):
                    structured_data.append(dict(zip(fieldnames, values)))
                else:
                    print(f"Skipping row with inconsistent data: {row}")

        return structured_data, len(structured_data)
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error generating synthetic data: {str(e)}"
        ) from e

def generate_additional_rows(example_data, num_rows, data_type, batch_size, existing_data):
    additional_rows_needed = num_rows - len(existing_data)
    if additional_rows_needed <= 0:
        return existing_data

    additional_data, _ = generate_synthetic_data(example_data, additional_rows_needed, data_type, batch_size)
    existing_data.extend(additional_data)
    return existing_data

def create_csv_file(structured_data, output_file):
    if not structured_data:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="No structured data to write to CSV."
        )
    try:
        fieldnames = list(structured_data[0].keys())

        with open(output_file, "w", newline="", encoding="utf-8") as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(structured_data)
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error creating CSV file: {str(e)}"
        ) from e

def infer_data_type(value):
    if value.isdigit():
        return "integer"
    elif value.replace(".", "", 1).isdigit():
        try:
            float(value)
            return "float"
        except ValueError:
            pass
    elif is_valid_date(value):
        return "date"
    else:
        return "string"

def is_valid_date(value):
    date_formats = ["%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y"]
    for date_format in date_formats:
        try:
            datetime.strptime(value, date_format)
            return True
        except ValueError:
            pass
    return False

def parse_and_validate_row(row, data_types):
    for field, value in row.items():
        if data_types[field] == "integer":
            try:
                row[field] = str(int(float(value)))
            except ValueError:
                row[field] = str(value)  # Keep the original value as a string if conversion fails
        elif data_types[field] == "float":
            try:
                row[field] = str(float(value))
            except ValueError:
                row[field] = str(value)  # Keep the original value as a string if conversion fails
        elif data_types[field] == "date":
            try:
                row[field] = parse_and_format_date(value)
            except ValueError:
                row[field] = str(value)  # Keep the original value as a string if parsing fails
    return row

def parse_and_format_date(value):
    date_formats = ["%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y"]
    for date_format in date_formats:
        try:
            date_obj = datetime.strptime(value, date_format)
            return date_obj.strftime("%Y-%m-%d")
        except ValueError:
            pass
    raise ValueError(f"Invalid date format: {value}")

def ensure_uniqueness(row, unique_values, data_type, fieldnames):
    for field in fieldnames:
        if field not in unique_values:
            unique_values[field] = set()
        if row[field] in unique_values[field]:
            if data_type == "timeseries" and field == fieldnames[0]:
                raise DuplicateTimestampError(message="Duplicate timestamp found in the generated time-series data.")
            else:
                row[field] += "_" + str(len(unique_values[field]) + 1)
        unique_values[field].add(row[field])
    return row

def sort_timeseries_data(data, fieldnames):
    timestamp_field = fieldnames[0]
    data.sort(key=lambda x: x[timestamp_field])
    return data

def evaluate_and_fix_csv(input_file, output_file, data_type):
    try:
        data = []
        unique_values = {}
        data_types = {}

        with open(input_file, "r", encoding="utf-8") as csvfile:
            reader = csv.DictReader(csvfile)
            fieldnames = reader.fieldnames

            for row in reader:
                for field, value in row.items():
                    if field not in data_types:
                        data_types[field] = infer_data_type(value)

                row = parse_and_validate_row(row, data_types)
                row = ensure_uniqueness(row, unique_values, data_type, fieldnames)
                data.append(row)

        unique_data = [dict(t) for t in {tuple(d.items()) for d in data}]

        if data_type == "timeseries":
            unique_data = sort_timeseries_data(unique_data, fieldnames)

        with open(output_file, "w", newline="", encoding="utf-8") as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(unique_data)
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error evaluating and fixing CSV file: {str(e)}"
        ) from e
